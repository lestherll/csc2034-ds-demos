{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE519E922OqB"
      },
      "source": [
        "# Linearly Separable Data\n",
        "## CSC2034\n",
        "### Cameron Trotter (c.trotter2@ncl.ac.uk)\n",
        "\n",
        "\n",
        "Welcome to the CSC2034 practicals. Throughout these sessions, you will learn how to use Python and the [sklearn package](https://scikit-learn.org/stable/) to perform basic data science, using models to infer relationships from data. I'd encourage you to make use of the demonstrators here, as well as explore things at your own pace. There are potentially multiple ways of performing operations in these notebooks, and Paolo will go through the theoretics in his lectures. Remeber, Google and Stack Overflow are your friend - just try to understand the code you find rather than blindly copying it in!\n",
        "\n",
        "Data science is an extremely broad topic, so let's begin with one of the simplest forms of data you might encounter. Linearly separable data is that which can be split using a straight (linear) line. \n",
        "\n",
        "### Google Colab Setup\n",
        "\n",
        "All of the notebooks you will be running in these lab sessions are designed to be ran using [Google Colab](https://colab.research.google.com/). For setup instructions, see this repo's README. \n",
        "\n",
        "In order to make things work on colab, we need to clone this repo and then (in another cell because colab dictates this...) move into the repo directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HfmSRqIG2OqF",
        "outputId": "86f77388-faca-4af8-da5c-b2137e82c361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'csc2034-ds-demos'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 127 (delta 49), reused 107 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (127/127), 3.66 MiB | 30.24 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Trotts/csc2034-ds-demos.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wbtAcDI-2OqH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('csc2034-ds-demos')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlEF0MZZ2OqH"
      },
      "source": [
        "### Building Synthetic Data\n",
        "\n",
        "As this is (probably) your first time doing any data science, it's important to have *nice* data. The vast majority of data you will work with if you decide to continue with data science is dirty, and requires a large amount of cleaning before you can do anything with it. That's a bit too much to deal with right now, so to start lets make a clean synthetic dataset. Thankfully, sklearn allows us to make these kinds of datasets with relative ease using the `make_classification` function. \n",
        "\n",
        "Your first task is to make some clean data to work with. I've included some parameters that I'd like your data to adhere to. Reading the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) might help here. For further reading see [here](https://towardsdatascience.com/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7afkN472OqI"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "\n",
        "n_samples = 1000\n",
        "n_classes = 2\n",
        "n_features = 2\n",
        "n_clusters_per_class = 2\n",
        "n_redundant = 0\n",
        "n_informative = 2\n",
        "random_state = 5\n",
        "flip_y = 0.1\n",
        "\n",
        "data, labels = make_classification(\n",
        "    nsamples=n_samples,\n",
        "    n_features=n_features,\n",
        "    n_informative=n_informative,\n",
        "    n_redundant=n_redundant,\n",
        "    n_classes=n_classes,\n",
        "    n_clusters_per_class=n_clusters_per_class,\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI7BRxC42OqI"
      },
      "source": [
        "Once created, `data` should be a numpy array of shape (n_samples, n_features), with each feature being a float. `labels` should be a numpy array of shape (1000,) representing the interger class labels of each datapoint, either 0 or 1.\n",
        "\n",
        "Run the below checks. If any return False, take another look at the code you have written before continuing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnsRYTbu2OqJ"
      },
      "outputs": [],
      "source": [
        "print(f\"data is of shape (n_samples, n_features): {data.shape == (n_samples, n_features)}\")\n",
        "print(f\"data is float: {data.dtype == 'float64'}\")\n",
        "print(f\"labels is of shape (n_samples,): {labels.shape == (n_samples,)}\")\n",
        "print(f\"labels is int: {labels.dtype == 'int64'}\")\n",
        "print(f\"Random state set correctly: {data[0].tolist() == [1.031573753611461, 0.9858931626901614]} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS5UPOrX2OqL"
      },
      "source": [
        "### Visualisation\n",
        "\n",
        "Being able to visually exmaine the data you have created is extremely important, so let's take a look at what we have. [Seaborn](https://seaborn.pydata.org/) is a powerful visualisation package built on top of [matplotlib](https://matplotlib.org/stable/users/index.html); let's use it. We're going to make lots of scatterplots here, so I've made a function to reduce code duplication, which can be found in `helpers.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-EBoZ5i2OqL"
      },
      "outputs": [],
      "source": [
        "from helpers import show_scatterplot\n",
        "\n",
        "show_scatterplot(data, labels, \"Scatterplot of synthetic data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQWshxMy2OqM"
      },
      "source": [
        "From the plot, we can see we have created a linearly separable dataset! We know this, as we can (pretty much) run a sraight line through the plot and separate the data. I say pretty much, as we created the data in such a way to have outliers on either side to make classification slightly more realistic. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLmZi4MO2OqN"
      },
      "source": [
        "### Spliting Into Sets\n",
        "\n",
        "Now that we have some data, we need to put it in a form usable to train a model on. For this, we need to split the data into a train and test set. In practice we might also split these further to create an evaluation set, but here we will keep it simple.\n",
        "\n",
        "As the names imply, we will utilise the train set to train our model and the test set to test its performance. It is _very_ important that we do not mix the sets, as using training data at test time can skew your metrics. Thankfully, sklearn has the ability to split the data for us through the `train_test_split` method. \n",
        "\n",
        "For your next task, use `train_test_split` to create two sets where the train set contains 80% of the entire dataset. The [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) may help you. \n",
        "\n",
        "NOTE: Make sure you set the `random_state` of the method to the same value as previously to ensure reproducability. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyvBADpC2OqN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_train, data_test, labels_train,  labels_test = #..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vprNh_OK2OqO"
      },
      "source": [
        "Once complete your train set should contain 80% of the whole dataset, with your test set containing the rest.\n",
        "\n",
        "Run the below checks. If any return False, take another look at the code you have written before continuing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcyDCG-Z2OqO"
      },
      "outputs": [],
      "source": [
        "print(f\"train set is 80% of dataset: {len(data_train) == n_samples * 0.8}\")\n",
        "print(f\"test set is 20% of dataset: {len(data_test) == n_samples * 0.2}\")\n",
        "print(f\"Random state set correctly: {data_train[0].tolist() == [-1.2833740134353988, -1.4154849762712642]} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2uaIKYu2OqP"
      },
      "source": [
        "### Data Scaling\n",
        "\n",
        "In the vast majority of cases, before we can use the splits we have created we will need to scale them. This processes forces our data to have a mean of 0 and a variance of 1, and is required to allow for optimal performance of many common machine learning models. If you want to see the negative effect not scaling your data can have, scikit-learn has a [section on the effects of not standardizing your data](\n",
        "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py).\n",
        "\n",
        "Sklearn provides the `StandardScaler` functionality which allows us to very quickly scale our splits to meet the required conditions. Unlike the other functionality provided by sklearn we have used so far, `StandardScaler` needs to be fitted to the training data, that is it needs to learn some parameters in order to operate. The [docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for `StandardScaler` provides more information regarding this. \n",
        "\n",
        "For your next task, scale the train and test splits you have created so that the data has a mean of 0 and a variance of 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK16J7242OqP"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#...\n",
        "\n",
        "data_train_scaled = #...\n",
        "data_test_scaled = #..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dklkFhhk2OqP"
      },
      "source": [
        "Run the below checks. If any return False, take another look at the code you have written before continuing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uknmB2W62OqP"
      },
      "outputs": [],
      "source": [
        "print(f\"train set has been correctly scaled: {data_train_scaled[0].tolist() == [-0.9534808523834634, -1.1352230069380507]}\")\n",
        "print(f\"test set has been correctly scaled: {data_test_scaled[0].tolist() == [1.2480977883641757, -0.8652814153775584]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSpwBcKW2OqQ"
      },
      "source": [
        "Now let's visualise our scaled splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rJxL4B02OqQ"
      },
      "outputs": [],
      "source": [
        "show_scatterplot(data_train_scaled, labels_train, \"Scatterplot of synthetic, scaled, train data\")\n",
        "show_scatterplot(data_test_scaled, labels_test, \"Scatterplot of synthetic, scaled, test data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAaSAd5i2OqQ"
      },
      "source": [
        "### Model Building\n",
        "\n",
        "Now that we have our synthetic data in a useable form, let's now look at how we train statistical models. There are a whole host of models provided by sklearn, but lets start with some of the most basic. Don't worry, we won't be going into the statistics behind how these models work - that will come if you decide to undertake optional modules in machine learning in later stages of your degree. \n",
        "\n",
        "#### Logistic Regression\n",
        "\n",
        "One of the most basic forms of machine learning is logistic regression. This method is very similar to linear regression, which you may have come across before if you have taken any stats modules in the past, except we output a binary variable rather than something continuous.\n",
        "\n",
        "Logistic regression works well for the data we have, as we are aiming to classify a data point into one of two classes. Sklearn provides functionality for logistic regression through the `LogisticRegression` class. Like `StandardScaler`, we need to fit the logistic regression model to the data.\n",
        "\n",
        "Task: Build a logistic regression model and train it on the data. All machine learning models take as input variables called hyperparameters. These are parameters which must be user defined rather than learned, and allow learning to take place. I have defined some hyperparameters for you. Once again, the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) will help you in building the model and passing the hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__Jqbf_c2OqQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "penalty = 'l2'\n",
        "C = 1\n",
        "solver = 'lbfgs'\n",
        "multi_class = 'ovr'\n",
        "\n",
        "logistic_regression = #...\n",
        "\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-9Qo3Dd2OqR"
      },
      "source": [
        "Now we have created a model and trained it, let's see what it looks like! The code for this is a bit complex, so I have provided it in `helpers.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE22pICI2OqR"
      },
      "outputs": [],
      "source": [
        "from helpers import plot_linear_fit\n",
        "\n",
        "plot_linear_fit(logistic_regression, data_train_scaled, labels_train,\n",
        "                \"Logistic regression line of best fit and training data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1XvSnAs2OqR"
      },
      "source": [
        "The red line in the plot above shows the line of best fit created by the logistic regression model. It's not great, but that's expected with the data and model we used. Let's see how it looks on the test set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UAoyzgx2OqR"
      },
      "outputs": [],
      "source": [
        "plot_linear_fit(logistic_regression, data_test_scaled, labels_test,\n",
        "                \"Logistic regression line of best fit and test data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CX9k-Ld2OqR"
      },
      "source": [
        "Noe we have a line of best and a trained model, we can use this to predict values for the test set data points. By doing this, we will be able to see how good our line of best fit is at correctly labelling our data. The model will produce predictions for each data point, and we can then compare these against the ground truth labels in the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJMlR8kg2OqS"
      },
      "outputs": [],
      "source": [
        "logistic_regression_label_predictions = logistic_regression.predict(data_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inIEHWT52OqS"
      },
      "source": [
        "We can also aquire some numerical metrics to show how the model is performing. One of the main ones is accuracy, which tells us how many data points in the set the model correctly labels. We take each data point in the set, predict a class for it using the model, and compare this to the ground truth label. \n",
        "\n",
        "Sklearn provides the ability to calculate these metrics using `accuracy_score`. The docs for the method are available [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html?highlight=accuracy_score#sklearn.metrics.accuracy_score).\n",
        "\n",
        "Task: Produce accuracy scores for the model you have created against test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCwr64Oo2OqS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "test_acc = #..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eK3TIAu2OqS"
      },
      "source": [
        "Let's check the accuracy score we achieve. We can display this as a percentage by multiplying by 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7kLbK-k2OqS"
      },
      "outputs": [],
      "source": [
        "print(f\"Logistic regression test accuracy: {test_acc * 100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh2a09ww2OqS"
      },
      "source": [
        "Run the below checks. If any return False, take another look at the code you have written before continuing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU2LTh_R2OqT"
      },
      "outputs": [],
      "source": [
        "print(f\"Test accuracy check: {test_acc == 0.835}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxoNY2JZ2OqT"
      },
      "source": [
        "There are lots of other metrics that sklearn can provide for us, such as precision, recall, and f-1 score. These can be produced using `classification_report`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyQspdlH2OqT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(labels_test, logistic_regression_label_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooc9hYZ2OqT"
      },
      "source": [
        "We can also produce a confusion matrix, which can provide us insight into which classes the model is mislabelling. These are especially useful when you have a multiclass model to understand how your model's accuracy can be improved. Sklearn's `conusion_matrix` method can product a textual confusion matrix for you, which can be visually enhanced using seaborn. I have made a function in `helpers` to do this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoVRH9d42OqT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from helpers import plot_confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "conf_matrix = confusion_matrix(labels_test, logistic_regression_label_predictions)\n",
        "# Normalise the data for better visualisation\n",
        "conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "labels = np.unique(labels_test)\n",
        "\n",
        "plot_confusion_matrix(conf_matrix, labels, \"Confusion Matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iCfXsxJ2OqU"
      },
      "source": [
        "This tells us that 85% of data points labelled as class 0 were correctly classified (True Negative) and 15% were incorrectly classified (False Positive). Further, 82% of data points labelled as class 1 were correctly classified (True Positive) and 18% were incorrectly classified (False Negative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE2RVv3N2OqU"
      },
      "source": [
        "We can also plot an ROC graph, which shows us the performance of the model at different thresholds. Again, these can be somewhat arduous to create, so code is provided for you in `helpers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MwGJ_G12OqU"
      },
      "outputs": [],
      "source": [
        "from helpers import plot_ROC\n",
        "\n",
        "plot_ROC(logistic_regression, data_test_scaled, labels_test, logistic_regression_label_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiRZNLpy2OqU"
      },
      "source": [
        "#### Support Vector Machine (SVM)\n",
        "\n",
        "There are lots of different models which a data scientist can utilise, each with their own strengths and weaknesses depening on the data. Thankfully sklearn contains multiple model implementations - one of which is an SVM. \n",
        "\n",
        "SVMs can be utilised for both linearly and non-linearly separable data, although sklearn dictates the use of two different functions depening. The documentation for a Linear SVM can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).\n",
        "\n",
        "Task: Using the following hyperparameters, implement a Linear SVM on the generated data.\n",
        "\n",
        "* C = 1\n",
        "* Loss = hinge\n",
        "\n",
        "Once you have created a model and trained it on the correct split, create predictions from the model and evaluate their performance using the methods utilised previously. As our data is synthetic, and quite simple, expect very similar model performance to your linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow18hOdl2OqU"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKvhHv3F2OqU"
      },
      "source": [
        "#### Decision Trees\n",
        "\n",
        "Decision Trees are another type of data science model. It utilises a tree structure to model outcomes based on provided values. \n",
        "\n",
        "Task: Create a decision tree using sklearn, fit it to your data, and generate evaluation metrics. You do not need to worry about hyperparameters here, nor do you need to use `plot_linear_fit` - instead use `plot_contour_fit`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlETiqTM2OqU"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "from helpers import plot_contour_fit\n",
        "\n",
        "#..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPfcU-po2OqV"
      },
      "source": [
        "We can visualise the decision tree using the `graphviz` package. Using this package is slightly easier than seaborn for this type of model. As our data is numeric, this will likely make little sense, but hopefully you will be able to see it's use when we have categorical data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "IIeBhZ_n2OqV"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "\n",
        "dot = tree.export_graphviz(decision_tree, out_file=None) \n",
        "graph = graphviz.Source(dot) \n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs17i6vr2OqV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "01-linearly-separable-data.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}